{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1",
      "metadata": {},
      "source": "# Group 18 \u2014 Dementia Prediction Project\n## Step 3: SMOTE Preprocessing + Train/Test Split\n\n> **Prerequisites:** Run Step 1 first to generate `dementia_clean.csv`\n\n### Bugs Fixed\n| # | Location | Bug | Fix |\n|---|----------|-----|-----|\n| 1 | Cell 2 | `!pip install imbalanced-learn` was commented out \u2014 `imblearn` never installed, causing `ModuleNotFoundError` | Made it an executable install cell |\n| 2 | Cell 6 | `study1_rundmc` and `study1_scans` are `bool` dtype (from `pd.get_dummies`) \u2014 SMOTE requires all-numeric input, causing a `ValueError` | Cast both columns to `int` before scaling/SMOTE |"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2",
      "metadata": {},
      "outputs": [],
      "source": "import subprocess, sys\n\ndef install(pkg):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"])\n\ninstall(\"scikit-learn\")\ninstall(\"imbalanced-learn\")\n\nprint(\"\u2713 scikit-learn and imbalanced-learn installed\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3",
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"\u2713 Libraries loaded successfully\")"
    },
    {
      "cell_type": "markdown",
      "id": "c4",
      "metadata": {},
      "source": "## 0. Load Data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5",
      "metadata": {},
      "outputs": [],
      "source": "df = pd.read_csv('dementia_clean.csv')\nprint(f\"Loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n# BUG FIX 2: study1_rundmc and study1_scans are bool dtype from pd.get_dummies.\n# SMOTE calls sklearn's NearestNeighbours internally and requires all-numeric arrays.\n# bool arrays raise a ValueError during fit_resample \u2014 cast to int to fix this.\nbool_cols = df.select_dtypes(include='bool').columns.tolist()\nif bool_cols:\n    df[bool_cols] = df[bool_cols].astype(int)\n    print(f\"\u2713 Cast bool columns to int: {bool_cols}\")\n\nprint(f\"\\nClass distribution:\")\nprint(df['dementia'].value_counts().rename({0.0: 'No Dementia', 1.0: 'Dementia'}))\ndf.head()"
    },
    {
      "cell_type": "markdown",
      "id": "c6",
      "metadata": {},
      "source": "## 1. Define Feature Sets\n\nTwo feature sets are prepared:\n- **LR features** \u2014 uses only `Global` to avoid multicollinearity between EF, PS, Global (for Logistic Regression)\n- **RF features** \u2014 includes all three cognitive scores (safe for tree-based models like Random Forest / XGBoost)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7",
      "metadata": {},
      "outputs": [],
      "source": "FEATURES_LR = [\n    'age', 'gender', 'educationyears',\n    'Global',                          # single cognitive score avoids multicollinearity\n    'diabetes', 'hypertension', 'hypercholesterolemia', 'smoking',\n    'Fazekas', 'lac_count', 'CMB_count',\n    'study1_rundmc', 'study1_scans'\n]\n\nFEATURES_RF = [\n    'age', 'gender', 'educationyears',\n    'EF', 'PS', 'Global',             # all three cognitive scores safe for tree models\n    'diabetes', 'hypertension', 'hypercholesterolemia', 'smoking',\n    'Fazekas', 'lac_count', 'CMB_count',\n    'study1_rundmc', 'study1_scans'\n]\n\nTARGET = 'dementia'\n\n# Verify all expected columns are present\nmissing = [f for f in FEATURES_RF if f not in df.columns]\nprint(f\"Missing columns: {missing if missing else 'None \u2014 all features present \u2713'}\")\nprint(f\"\\nLR feature set ({len(FEATURES_LR)}): {FEATURES_LR}\")\nprint(f\"\\nRF feature set ({len(FEATURES_RF)}): {FEATURES_RF}\")"
    },
    {
      "cell_type": "markdown",
      "id": "c8",
      "metadata": {},
      "source": "## 2. Train / Test Split\n\n> \u26a0\ufe0f **SMOTE must only be applied to training data.**  \n> Splitting *before* SMOTE ensures no synthetic samples leak into the test set, which would cause artificially inflated evaluation metrics.\n\nA stratified 80/20 split preserves the ~4.5% dementia rate in both partitions."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9",
      "metadata": {},
      "outputs": [],
      "source": "X = df[FEATURES_RF]\ny = df[TARGET].astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y        # preserves dementia ratio in both splits\n)\n\nprint(f\"Train set: {X_train.shape[0]} samples\")\nprint(f\"Test set:  {X_test.shape[0]} samples\")\nprint(f\"\\nClass distribution BEFORE SMOTE:\")\nprint(f\"  Train \u2014 No Dementia: {(y_train==0).sum():4d}, \"\n      f\"Dementia: {(y_train==1).sum():3d} ({y_train.mean()*100:.1f}% positive)\")\nprint(f\"  Test  \u2014 No Dementia: {(y_test==0).sum():4d}, \"\n      f\"Dementia: {(y_test==1).sum():3d} ({y_test.mean()*100:.1f}% positive)\")"
    },
    {
      "cell_type": "markdown",
      "id": "c10",
      "metadata": {},
      "source": "## 3. Feature Scaling\n\n`StandardScaler` transforms each feature to mean = 0, std = 1.\n\n- **Required** for Logistic Regression (sensitive to feature magnitude)\n- **Best practice** for all models when features are on different scales\n- \u26a0\ufe0f Scaler is **fit on training data only**, then applied to test data \u2014 prevents data leakage"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11",
      "metadata": {},
      "outputs": [],
      "source": "scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)       # fit + transform on train\nX_test_scaled  = scaler.transform(X_test)            # transform only on test \u2014 DO NOT refit\n\n# Convert back to DataFrames to preserve column names\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=FEATURES_RF)\nX_test_scaled  = pd.DataFrame(X_test_scaled,  columns=FEATURES_RF)\n\nprint(\"\u2713 Scaling complete (fit on train only)\")\nprint(f\"  Train mean (should be ~0): {X_train_scaled.mean().round(3).values}\")\nprint(f\"  Test  mean (may deviate):  {X_test_scaled.mean().round(3).values}\")"
    },
    {
      "cell_type": "markdown",
      "id": "c12",
      "metadata": {},
      "source": "## 4. Apply SMOTE (Training Set Only)\n\n**SMOTE** (Synthetic Minority Oversampling Technique) creates synthetic dementia cases by interpolating between existing minority-class samples in feature space.\n\nResult: balanced 50/50 training set, while the test set retains the real-world distribution."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13",
      "metadata": {},
      "outputs": [],
      "source": "smote = SMOTE(random_state=42)\nX_train_sm, y_train_sm = smote.fit_resample(X_train_scaled, y_train)\n\nprint(\"Class distribution AFTER SMOTE:\")\nprint(f\"  Train \u2014 No Dementia: {(y_train_sm==0).sum()}, \"\n      f\"Dementia: {(y_train_sm==1).sum()} ({y_train_sm.mean()*100:.1f}% positive)\")\nprint(f\"\\n\u2713 Test set is NOT resampled (real-world distribution preserved)\")\nprint(f\"  Test  \u2014 No Dementia: {(y_test==0).sum()}, \"\n      f\"Dementia: {(y_test==1).sum()} ({y_test.mean()*100:.1f}% positive)\")"
    },
    {
      "cell_type": "markdown",
      "id": "c14",
      "metadata": {},
      "source": "## 5. Prepare Logistic Regression Feature Subset"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15",
      "metadata": {},
      "outputs": [],
      "source": "# LR uses only Global (not EF/PS) to avoid multicollinearity\nLR_COLS = [f for f in FEATURES_LR if f in X_train_sm.columns]\nX_train_lr = X_train_sm[LR_COLS]\nX_test_lr  = X_test_scaled[LR_COLS]\n\nprint(f\"LR feature set ({len(LR_COLS)} features): {LR_COLS}\")\nprint(f\"RF feature set ({len(FEATURES_RF)} features): {FEATURES_RF}\")"
    },
    {
      "cell_type": "markdown",
      "id": "c16",
      "metadata": {},
      "source": "## 6. Visualise Class Balance Before vs After SMOTE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17",
      "metadata": {},
      "outputs": [],
      "source": "colors = ['#4C72B0', '#DD8452']\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nfig.suptitle('Class Balance: Before vs After SMOTE (Training Set Only)',\n             fontsize=13, fontweight='bold')\n\nfor ax, counts, title in zip(\n    axes,\n    [[(y_train==0).sum(),    (y_train==1).sum()],\n     [(y_train_sm==0).sum(), (y_train_sm==1).sum()]],\n    ['Before SMOTE', 'After SMOTE']\n):\n    bars = ax.bar(['No Dementia', 'Dementia'], counts, color=colors, edgecolor='white')\n    ax.set_title(title, fontweight='bold')\n    ax.set_ylabel('Count')\n    for i, v in enumerate(counts):\n        ax.text(i, v + 5, f'n={v}\\n({v/sum(counts)*100:.1f}%)', ha='center', fontsize=10)\n    ax.set_ylim(0, max(counts) * 1.25)\n\nplt.tight_layout()\nplt.savefig('SMOTE_class_balance.png', dpi=150, bbox_inches='tight', facecolor='white')\nplt.show()\nprint(\"\u2713 Saved: SMOTE_class_balance.png\")"
    },
    {
      "cell_type": "markdown",
      "id": "c18",
      "metadata": {},
      "source": "## 7. Save Preprocessed Splits"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19",
      "metadata": {},
      "outputs": [],
      "source": "X_train_sm.to_csv('X_train_rf.csv',    index=False)  # RF/XGBoost train (SMOTE applied)\nX_test_scaled.to_csv('X_test_rf.csv',  index=False)  # RF/XGBoost test\nX_train_lr.to_csv('X_train_lr.csv',    index=False)  # LR train (SMOTE + subset)\nX_test_lr.to_csv('X_test_lr.csv',      index=False)  # LR test (subset)\npd.Series(y_train_sm, name='dementia').to_csv('y_train.csv', index=False)\npd.Series(y_test,     name='dementia').to_csv('y_test.csv',  index=False)\n\nprint(\"\u2713 Preprocessed splits saved:\")\nprint(\"   X_train_rf.csv / X_test_rf.csv  \u2192 Random Forest & XGBoost\")\nprint(\"   X_train_lr.csv / X_test_lr.csv  \u2192 Logistic Regression\")\nprint(\"   y_train.csv    / y_test.csv      \u2192 Labels\")\nprint(\"\\n\u2705 SMOTE preprocessing complete! Next: run Step 4 modelling notebook.\")"
    },
    {
      "cell_type": "markdown",
      "id": "c20",
      "metadata": {},
      "source": "## Summary\n\n| Output | Description |\n|--------|-------------|\n| `SMOTE_class_balance.png` | Before/after SMOTE bar charts |\n| `X_train_rf.csv` | RF/XGBoost training features (SMOTE-balanced) |\n| `X_test_rf.csv` | RF/XGBoost test features |\n| `X_train_lr.csv` | LR training features (SMOTE-balanced, no EF/PS) |\n| `X_test_lr.csv` | LR test features |\n| `y_train.csv` | Training labels |\n| `y_test.csv` | Test labels |"
    }
  ]
}